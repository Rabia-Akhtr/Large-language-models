{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xlPXZ_qhnKk"
      },
      "source": [
        "# Assignment 3 - LLM Fine-tuning for Text Classification (AG News)\n",
        "\n",
        "## Notebook checklist\n",
        "-  Public dataset + task definition (AG News, 4-class topic classification)\n",
        "- Preprocessing: BERT tokenizer + train/test usage (+ our train/val split)\n",
        "-  Baseline model(s): TF-IDF + Logistic Regression (and optional “pretrained BERT only” baseline)\n",
        "- Fine-tune BERT-style model with torch training loop + optimizer + loss\n",
        "-  Evaluation: accuracy, precision, recall, F1 + confusion matrix\n",
        "-  Model selection on validation only (no test leakage)\n",
        "- Reproducibility artifacts saved (config + results + predictions)\n",
        "- Error analysis (misclassified examples)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpI33FjyID-l"
      },
      "source": [
        "## **Install + imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q5B1TMSH7hg"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip -q install -U transformers datasets evaluate accelerate scikit-learn matplotlib\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from itertools import product\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import DatasetDict, load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    precision_recall_fscore_support,\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p36ooetFINGL"
      },
      "source": [
        "## **Reproducibility + device**\n",
        "\n",
        "**Goal:** Make results repeatable (same split, same training behavior).\n",
        "\n",
        "**What this does:**\n",
        "- Sets seeds for Python/NumPy/PyTorch.\n",
        "- Uses deterministic CuDNN settings.\n",
        "- Selects GPU if available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz83c56li9Gv"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42) -> None:\n",
        "    \"\"\"Set random seeds for reproducible experiments.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "print(\"Torch version:\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohps03gzIUPT"
      },
      "source": [
        "## **Load dataset (AG News)**\n",
        "\n",
        "**Task:** 4-class topic classification.\n",
        "\n",
        "**Dataset splits:**\n",
        "- Use official `train` and `test` splits from AG News.\n",
        "- Later,  created a **validation split from train only** (to avoid test leakage).\n",
        "\n",
        "**Evidence shown:**\n",
        "- dataset sizes\n",
        "- label names\n",
        "- example samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFq4bY05kJJE"
      },
      "outputs": [],
      "source": [
        "def load_ag_news() -> Tuple[DatasetDict, List[str]]:\n",
        "    \"\"\"Load AG News dataset and return dataset + label names.\"\"\"\n",
        "    ds = load_dataset(\"ag_news\")\n",
        "    label_names_local = ds[\"train\"].features[\"label\"].names\n",
        "    return ds, label_names_local\n",
        "\n",
        "\n",
        "dataset, label_names = load_ag_news()\n",
        "\n",
        "print(\"Dataset:\", dataset)\n",
        "print(\"Train rows:\", len(dataset[\"train\"]))\n",
        "print(\"Test rows :\", len(dataset[\"test\"]))\n",
        "print(\"Label names:\", label_names)\n",
        "\n",
        "for i in range(3):\n",
        "    print(\"\\nSample\", i)\n",
        "    print(\"Label:\", label_names[dataset[\"train\"][i][\"label\"]])\n",
        "    print(\"Text :\", dataset[\"train\"][i][\"text\"][:250], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJqmd563IoNo"
      },
      "source": [
        "## **Plot class distribution**\n",
        "\n",
        "**Why:** Confirms class balance and supports interpretation of macro metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTtGQERl5pUM"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import DatasetDict\n",
        "\n",
        "\n",
        "def plot_class_distribution(\n",
        "    ds: DatasetDict,\n",
        "    class_names: List[str],\n",
        "    *,\n",
        "    save_path: str = \"agnews_class_distribution_single_col.png\",\n",
        "    show_percent: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Plot AG News class distribution (Train vs Test) in a Word 2-column friendly style.\n",
        "\n",
        "    Changes from your version:\n",
        "    - Removed the top subtitle (Train n=..., Test n=...)\n",
        "    - Smaller, cleaner legend\n",
        "    - Kept larger fonts + high DPI for Word\n",
        "    \"\"\"\n",
        "    train_counts = (\n",
        "        pd.Series(ds[\"train\"][\"label\"])\n",
        "        .value_counts()\n",
        "        .sort_index()\n",
        "        .reindex(range(len(class_names)), fill_value=0)\n",
        "    )\n",
        "    test_counts = (\n",
        "        pd.Series(ds[\"test\"][\"label\"])\n",
        "        .value_counts()\n",
        "        .sort_index()\n",
        "        .reindex(range(len(class_names)), fill_value=0)\n",
        "    )\n",
        "\n",
        "    summary = pd.DataFrame(\n",
        "        {\n",
        "            \"label_id\": list(range(len(class_names))),\n",
        "            \"label_name\": class_names,\n",
        "            \"train_count\": train_counts.values,\n",
        "            \"test_count\": test_counts.values,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    summary[\"train_pct\"] = 100.0 * summary[\"train_count\"] / max(\n",
        "        1, summary[\"train_count\"].sum()\n",
        "    )\n",
        "    summary[\"test_pct\"] = 100.0 * summary[\"test_count\"] / max(\n",
        "        1, summary[\"test_count\"].sum()\n",
        "    )\n",
        "\n",
        "    y_train = summary[\"train_pct\"] if show_percent else summary[\"train_count\"]\n",
        "    y_test = summary[\"test_pct\"] if show_percent else summary[\"test_count\"]\n",
        "\n",
        "    # Fonts tuned for Word 2-column insertion\n",
        "    title_fs = 12\n",
        "    label_fs = 12\n",
        "    tick_fs = 12\n",
        "    legend_fs = 7     # smaller legend\n",
        "    annot_fs = 10\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 4.2))\n",
        "\n",
        "    x = np.arange(len(class_names))\n",
        "    width = 0.38\n",
        "\n",
        "    bars_train = ax.bar(x - width / 2, y_train, width=width, label=\"Train\")\n",
        "    bars_test = ax.bar(x + width / 2, y_test, width=width, label=\"Test\")\n",
        "\n",
        "    unit = \"Percentage of split (%)\" if show_percent else \"Number of articles\"\n",
        "    ax.set_title(\"AG News: class distribution (Train vs Test)\", fontsize=title_fs, pad=8)\n",
        "    ax.set_xlabel(\"class\", fontsize=label_fs)\n",
        "    ax.set_ylabel(unit, fontsize=label_fs)\n",
        "\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(summary[\"label_name\"], fontsize=tick_fs)\n",
        "    ax.tick_params(axis=\"y\", labelsize=tick_fs)\n",
        "\n",
        "    # Annotate values on bars\n",
        "    for bars in (bars_train, bars_test):\n",
        "        for b in bars:\n",
        "            h = float(b.get_height())\n",
        "            if h <= 0:\n",
        "                continue\n",
        "            label = f\"{h:.1f}%\" if show_percent else f\"{int(h):,}\"\n",
        "            ax.text(\n",
        "                b.get_x() + b.get_width() / 2,\n",
        "                h,\n",
        "                label,\n",
        "                ha=\"center\",\n",
        "                va=\"bottom\",\n",
        "                fontsize=annot_fs,\n",
        "            )\n",
        "\n",
        "    # Smaller legend, tucked into corner\n",
        "    ax.legend(\n",
        "        loc=\"upper right\",\n",
        "        fontsize=legend_fs,\n",
        "        frameon=True,\n",
        "        borderpad=0.3,\n",
        "        labelspacing=0.3,\n",
        "        handlelength=1.2,\n",
        "        handletextpad=0.4,\n",
        "    )\n",
        "\n",
        "    ax.margins(y=0.18)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(save_path, dpi=400, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Saved figure to: {save_path}\")\n",
        "    return summary\n",
        "\n",
        "\n",
        "label_summary = plot_class_distribution(\n",
        "    dataset,\n",
        "    label_names,\n",
        "    save_path=\"agnews_class_distribution_single_col_counts.png\",\n",
        "    show_percent=False,\n",
        ")\n",
        "label_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q9KkZ5bIwXC"
      },
      "source": [
        "## **Metrics (macro + accuracy)**\n",
        "\n",
        "**Requirement:** report accuracy, precision, recall, F1.  \n",
        "**Choice:** Macro averages treat each class equally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9KBvQ2lklA4"
      },
      "outputs": [],
      "source": [
        "def compute_macro_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"Compute accuracy and macro-averaged precision/recall/F1.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\")\n",
        "    return {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"macro_precision\": float(prec),\n",
        "        \"macro_recall\": float(rec),\n",
        "        \"macro_f1\": float(f1),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fBG9FLqJFbf"
      },
      "source": [
        "## **Config + reproducibility artifacts**\n",
        "\n",
        "Saved:\n",
        "- `config.json` (all key settings)\n",
        "- `grid_results.csv` (hyperparameter tuning outcomes)\n",
        "- `history.csv` (training curve values)\n",
        "- `best_model.pt` (best checkpoint state_dict)\n",
        "- `test_predictions.csv` (true/pred labels for audit)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-7-JX3HAil5"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TrainConfig:\n",
        "    \"\"\"Configuration for tokenization, splitting, and training.\"\"\"\n",
        "    model_name: str = \"bert-base-uncased\"\n",
        "    max_length: int = 128\n",
        "    val_ratio: float = 0.20\n",
        "    batch_size: int = 16\n",
        "    seed: int = 42\n",
        "    output_dir: str = \"artifacts_agnews\"\n",
        "\n",
        "\n",
        "CFG = TrainConfig()\n",
        "os.makedirs(CFG.output_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(CFG.output_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(CFG.__dict__, f, indent=2)\n",
        "\n",
        "num_labels = len(label_names)\n",
        "print(\"Saved config to:\", os.path.join(CFG.output_dir, \"config.json\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh5yzwrfI9DX"
      },
      "source": [
        "## **Baseline: TF-IDF + Logistic Regression (test evaluation)**\n",
        "\n",
        "**Why baseline:** classical ML reference point for the fine-tuned Transformer.  \n",
        "**No leakage:** baseline trains on train split, evaluates on official test split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUHCvSWA4yae"
      },
      "outputs": [],
      "source": [
        "def run_baseline_tfidf_logreg(\n",
        "    ds: DatasetDict,\n",
        "    target_names: List[str],\n",
        "    output_dir: str,\n",
        "    show_confusion_matrix: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Train TF-IDF + Logistic Regression baseline and evaluate on the official test split.\n",
        "\n",
        "    Artifact:\n",
        "        Saves baseline predictions for reproducibility/audit.\n",
        "    \"\"\"\n",
        "    x_train = ds[\"train\"][\"text\"]\n",
        "    y_train = np.array(ds[\"train\"][\"label\"])\n",
        "\n",
        "    x_test = ds[\"test\"][\"text\"]\n",
        "    y_test = np.array(ds[\"test\"][\"label\"])\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=50_000,\n",
        "        ngram_range=(1, 2),\n",
        "        lowercase=True,\n",
        "        stop_words=\"english\",\n",
        "    )\n",
        "    x_train_vec = vectorizer.fit_transform(x_train)\n",
        "    x_test_vec = vectorizer.transform(x_test)\n",
        "\n",
        "    try:\n",
        "        model = LogisticRegression(\n",
        "            max_iter=200,\n",
        "            solver=\"lbfgs\",\n",
        "            n_jobs=-1,\n",
        "            multi_class=\"multinomial\",\n",
        "        )\n",
        "    except TypeError:\n",
        "        model = LogisticRegression(max_iter=200, solver=\"lbfgs\")\n",
        "\n",
        "    model.fit(x_train_vec, y_train)\n",
        "    y_pred = model.predict(x_test_vec)\n",
        "\n",
        "    metrics = compute_macro_metrics(y_test, y_pred)\n",
        "\n",
        "    print(\"Baseline (TF-IDF + Logistic Regression) — TEST:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k:>16}: {v:.4f}\")\n",
        "\n",
        "    print(\"\\nClassification report (baseline):\")\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "    if show_confusion_matrix:\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        plt.imshow(cm)\n",
        "        plt.title(\"Confusion Matrix (Baseline) — Test Set\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"True\")\n",
        "        plt.xticks(range(len(target_names)), target_names, rotation=30, ha=\"right\")\n",
        "        plt.yticks(range(len(target_names)), target_names)\n",
        "        plt.colorbar()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    preds_path = os.path.join(output_dir, \"baseline_predictions.csv\")\n",
        "    pd.DataFrame({\"y_true\": y_test, \"y_pred\": y_pred}).to_csv(preds_path, index=False)\n",
        "    print(\"Saved:\", preds_path)\n",
        "\n",
        "    return {\"metrics\": metrics, \"y_true\": y_test, \"y_pred\": y_pred}\n",
        "\n",
        "\n",
        "baseline_output = run_baseline_tfidf_logreg(\n",
        "    ds=dataset,\n",
        "    target_names=label_names,\n",
        "    output_dir=CFG.output_dir,\n",
        ")\n",
        "baseline_metrics = baseline_output[\"metrics\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUcNO9ZGJKzR"
      },
      "source": [
        "## **Tokenization + DataLoaders (train/val/test)**\n",
        "\n",
        "**Requirement:** tokenize text using a BERT tokenizer.  \n",
        "**No leakage:** validation is split from train; test is untouched until final evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZgrIEHXljtL"
      },
      "outputs": [],
      "source": [
        "TOKENIZED_CACHE: Dict[Tuple[str, int], DatasetDict] = {}\n",
        "\n",
        "\n",
        "def clear_gpu_memory() -> None:\n",
        "    \"\"\"Free CUDA cached memory to reduce OOM risk.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def prepare_tokenized_splits_and_loaders(\n",
        "    ds: DatasetDict,\n",
        "    model_name: str,\n",
        "    max_length: int,\n",
        "    batch_size: int,\n",
        "    val_ratio: float,\n",
        "    seed: int,\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader, AutoTokenizer]:\n",
        "    \"\"\"Tokenize dataset and create train/val/test DataLoaders.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    cache_key = (model_name, max_length)\n",
        "\n",
        "    if cache_key not in TOKENIZED_CACHE:\n",
        "\n",
        "        def _tokenize(batch: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            return tokenizer(batch[\"text\"], truncation=True, max_length=max_length)\n",
        "\n",
        "        tokenized = ds.map(_tokenize, batched=True)\n",
        "        tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "        tokenized = tokenized.remove_columns([\"text\"])\n",
        "        tokenized.set_format(\"torch\")\n",
        "        TOKENIZED_CACHE[cache_key] = tokenized\n",
        "        print(\"✅ Tokenization cached for:\", cache_key)\n",
        "    else:\n",
        "        tokenized = TOKENIZED_CACHE[cache_key]\n",
        "\n",
        "    split = tokenized[\"train\"].train_test_split(test_size=val_ratio, seed=seed)\n",
        "    train_ds = split[\"train\"]\n",
        "    val_ds = split[\"test\"]\n",
        "    test_ds = tokenized[\"test\"]\n",
        "\n",
        "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0WvKFcZOFJv"
      },
      "source": [
        "## Model Architecture and Training Pipeline (BERT Fine-tuning)\n",
        "\n",
        "**Model chosen:** `bert-base-uncased` (BERT encoder + classification head)  \n",
        "**Task:** 4-class topic classification (AG News: World, Sports, Business, Sci/Tech)\n",
        "\n",
        "### Why BERT is appropriate for this problem\n",
        "BERT is an encoder-only Transformer pretrained to learn rich contextual word representations.\n",
        "For topic classification, context matters because the same token can have different meanings\n",
        "depending on surrounding words (e.g., \"Apple\" company vs fruit). BERT’s self-attention\n",
        "captures long-range dependencies and produces a strong sequence representation for classification.\n",
        "\n",
        "### Architecture (high level)\n",
        "- Input text is tokenized with **BERT WordPiece tokenizer**.\n",
        "- The tokenized sequence is passed through the **BERT encoder**.\n",
        "- The special token `[CLS]` represents the whole sequence.\n",
        "- A **classification head** (linear layer) maps the `[CLS]` embedding to 4 output logits.\n",
        "- Training minimizes **cross-entropy loss** for multi-class classification.\n",
        "\n",
        "### Pipeline diagram\n",
        "\n",
        "Raw text\n",
        "  ↓\n",
        "BERT tokenizer (WordPiece) + truncation (max_length) + dynamic padding per batch\n",
        "  ↓\n",
        "input_ids, attention_mask, token_type_ids\n",
        "  ↓\n",
        "BERT encoder (12 Transformer layers: self-attention + feed-forward)\n",
        "  ↓\n",
        "[CLS] pooled embedding (sequence-level representation)\n",
        "  ↓\n",
        "Classification head (dropout + linear layer)\n",
        "  ↓\n",
        "Logits for 4 classes\n",
        "  ↓\n",
        "CrossEntropyLoss (computed internally when labels are provided)\n",
        "\n",
        "### Key hyperparameters (and why)\n",
        "- **max_length:** controls how much context the model reads (trade-off: context vs compute)\n",
        "- **batch_size:** controls gradient stability and GPU memory usage\n",
        "- **learning_rate:** most sensitive fine-tuning parameter (too high can destabilize training)\n",
        "- **weight_decay:** regularization to reduce overfitting\n",
        "- **warmup_ratio:** stabilizes early training by slowly increasing the learning rate\n",
        "- **early stopping + best checkpoint:** selects the best epoch using validation Macro-F1 and\n",
        "  prevents overfitting; test set is only used once for final evaluation (no leakage)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLQOTQIHJabZ"
      },
      "source": [
        "## **Build LLM model (BERT for sequence classification)**\n",
        "\n",
        "I use `AutoModelForSequenceClassification` which attaches a classification head on top of BERT.\n",
        "\n",
        "**Input:** token IDs + attention mask (dynamic padding per batch).  \n",
        "**Output:** logits for 4 classes and cross-entropy loss (when labels are provided).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "895uKL7Ql__G"
      },
      "outputs": [],
      "source": [
        "def build_model(model_name: str, num_labels: int) -> torch.nn.Module:\n",
        "    \"\"\"Create a BERT-style sequence classification model with `num_labels` outputs.\"\"\"\n",
        "    return AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96rQSph0JkB1"
      },
      "source": [
        "## **Evaluation (validation/test)**\n",
        "\n",
        "**Requirement:** report accuracy, precision, recall, F1.  \n",
        "I compute:\n",
        "- Weighted-average loss (correct even if last batch is smaller)\n",
        "- Macro metrics from predictions\n",
        "\n",
        "**No leakage:** validation is used for model selection; test is used once at the end.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhW7raekmS2v"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model: torch.nn.Module, dataloader: DataLoader) -> Dict[str, Any]:\n",
        "    \"\"\"Evaluate model and return weighted average loss + macro metrics + predictions.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_examples = 0\n",
        "    preds_list: List[np.ndarray] = []\n",
        "    labels_list: List[np.ndarray] = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        batch_size = int(batch[\"labels\"].size(0))\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        total_loss += float(loss.item()) * batch_size\n",
        "        total_examples += batch_size\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "        preds_list.append(preds.detach().cpu().numpy())\n",
        "        labels_list.append(batch[\"labels\"].detach().cpu().numpy())\n",
        "\n",
        "    y_pred = np.concatenate(preds_list)\n",
        "    y_true = np.concatenate(labels_list)\n",
        "\n",
        "    metrics = compute_macro_metrics(y_true, y_pred)\n",
        "    avg_loss = total_loss / max(1, total_examples)\n",
        "\n",
        "    return {\"loss\": avg_loss, \"metrics\": metrics, \"y_true\": y_true, \"y_pred\": y_pred}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_cgtvKIJqr6"
      },
      "source": [
        "## **Training loop**\n",
        "\n",
        "I fine-tune all model parameters using:\n",
        "- AdamW optimizer\n",
        "- Linear LR schedule with warmup\n",
        "- Gradient clipping for stability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMJZze8H7t2k"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: Optional[Any] = None,\n",
        "    grad_clip_norm: float = 1.0,\n",
        "    use_amp: bool = True,\n",
        "    grad_accum_steps: int = 1,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Train for one epoch and return weighted average training loss.\n",
        "\n",
        "    Notes:\n",
        "    - Uses mixed precision (AMP) on CUDA to speed up training and reduce memory.\n",
        "    - Uses gradient accumulation to simulate larger effective batch sizes.\n",
        "    - Hugging Face computes CrossEntropyLoss internally when `labels` are present.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    if grad_accum_steps < 1:\n",
        "        raise ValueError(\"grad_accum_steps must be >= 1.\")\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    amp_enabled = bool(use_amp and use_cuda)\n",
        "\n",
        "    # New AMP API (avoids FutureWarning)\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=amp_enabled)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, batch in enumerate(dataloader, start=1):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        batch_size = int(batch[\"labels\"].size(0))\n",
        "\n",
        "        # Forward pass under autocast\n",
        "        with torch.amp.autocast(\"cuda\", enabled=amp_enabled):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss / grad_accum_steps\n",
        "\n",
        "        # Backward (scaled if AMP enabled)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Step optimizer every grad_accum_steps\n",
        "        if step % grad_accum_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_norm)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Track true (non-divided) loss for reporting\n",
        "        total_loss += float(loss.item()) * batch_size * grad_accum_steps\n",
        "        total_examples += batch_size\n",
        "\n",
        "    # Handle leftover grads if steps not divisible by grad_accum_steps\n",
        "    if len(dataloader) % grad_accum_steps != 0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip_norm)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    return total_loss / max(1, total_examples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSJ2YzY7J2lr"
      },
      "source": [
        "## **Train with early stopping (selection on validation macro-F1)**\n",
        "\n",
        "**Model selection rule:** keep the checkpoint with the best validation Macro-F1.\n",
        "\n",
        "**Early stopping:** stop if Macro-F1 does not improve for `patience` epochs.\n",
        "\n",
        "**No leakage:** test set is not used here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y11ZogCFha6-"
      },
      "outputs": [],
      "source": [
        "def train_with_early_stop_and_best_ckpt(\n",
        "    model_name: str,\n",
        "    num_labels: int,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    lr: float,\n",
        "    epochs: int,\n",
        "    weight_decay: float,\n",
        "    warmup_ratio: float,\n",
        "    patience: int,\n",
        "    grad_clip_norm: float,\n",
        "    seed: int,\n",
        "    use_amp: bool = True,\n",
        "    grad_accum_steps: int = 1,\n",
        ") -> Tuple[float, Dict[str, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Train and early-stop using VAL macro-F1.\n",
        "\n",
        "    Returns:\n",
        "        best_val_macro_f1, best_checkpoint_state_dict (CPU)\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    model = build_model(model_name, num_labels).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # ✅ Correct scheduler step count when using gradient accumulation\n",
        "    steps_per_epoch = (len(train_loader) + grad_accum_steps - 1) // max(1, grad_accum_steps)\n",
        "    total_steps = steps_per_epoch * epochs\n",
        "    warmup_steps = int(warmup_ratio * total_steps)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "\n",
        "    best_val_f1 = -1.0\n",
        "    best_state_dict: Optional[Dict[str, torch.Tensor]] = None\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train_one_epoch(\n",
        "            model=model,\n",
        "            dataloader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            grad_clip_norm=grad_clip_norm,\n",
        "            use_amp=use_amp,\n",
        "            grad_accum_steps=grad_accum_steps,\n",
        "        )\n",
        "\n",
        "        val_out = evaluate_model(model, val_loader)\n",
        "        val_loss = float(val_out[\"loss\"])\n",
        "        val_f1 = float(val_out[\"metrics\"][\"macro_f1\"])\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{epochs} | \"\n",
        "            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} \"\n",
        "            f\"val_macro_f1={val_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_state_dict = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping triggered (no improvement on val Macro-F1).\")\n",
        "            break\n",
        "\n",
        "    if best_state_dict is None:\n",
        "        best_state_dict = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "    return best_val_f1, best_state_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWPWshmAKAAo"
      },
      "source": [
        "## **Grid search (validation-only selection) + save results**\n",
        "\n",
        "We run a small hyperparameter grid due to compute limits and time saving.\n",
        "\n",
        "**Saved artifact:** `grid_results.csv` in `CFG.output_dir`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qasNvSk35WvJ"
      },
      "outputs": [],
      "source": [
        "set_seed(CFG.seed)\n",
        "clear_gpu_memory()\n",
        "\n",
        "learning_rates = [2e-5, 3e-5]\n",
        "batch_sizes = [16, 32]\n",
        "epochs_to_try = [3, 4]\n",
        "weight_decays = [0.0, 0.01]\n",
        "\n",
        "warmup_ratio = 0.10\n",
        "grad_clip_norm = 1.0\n",
        "patience = 2\n",
        "\n",
        "# Speed knobs (recommended for your slow training)\n",
        "use_amp = True\n",
        "grad_accum_steps = 1  # set to 2 if you must use batch_size=16 but want stability\n",
        "\n",
        "results: List[Dict[str, float]] = []\n",
        "best_config: Optional[Dict[str, float]] = None\n",
        "best_val_f1_overall = -1.0\n",
        "\n",
        "print(\"Starting grid search (selection on VAL only)...\")\n",
        "\n",
        "for lr, bs, ep, wd in product(learning_rates, batch_sizes, epochs_to_try, weight_decays):\n",
        "    print(\"\\n-------------------------\")\n",
        "    print(f\"Config: lr={lr}, bs={bs}, epochs={ep}, weight_decay={wd}\")\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "    try:\n",
        "        clear_gpu_memory()\n",
        "\n",
        "        train_loader, val_loader, _, _ = prepare_tokenized_splits_and_loaders(\n",
        "            ds=dataset,\n",
        "            model_name=CFG.model_name,\n",
        "            max_length=CFG.max_length,\n",
        "            batch_size=bs,\n",
        "            val_ratio=CFG.val_ratio,\n",
        "            seed=CFG.seed,\n",
        "        )\n",
        "\n",
        "        val_f1, _ = train_with_early_stop_and_best_ckpt(\n",
        "            model_name=CFG.model_name,\n",
        "            num_labels=num_labels,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            lr=float(lr),\n",
        "            epochs=int(ep),\n",
        "            weight_decay=float(wd),\n",
        "            warmup_ratio=float(warmup_ratio),\n",
        "            patience=int(patience),\n",
        "            grad_clip_norm=float(grad_clip_norm),\n",
        "            seed=CFG.seed,\n",
        "            use_amp=use_amp,\n",
        "            grad_accum_steps=grad_accum_steps,\n",
        "        )\n",
        "\n",
        "    except RuntimeError as err:\n",
        "        if \"out of memory\" in str(err).lower():\n",
        "            print(\"Skipped: CUDA OOM\")\n",
        "            clear_gpu_memory()\n",
        "            continue\n",
        "        raise\n",
        "\n",
        "    row = {\n",
        "        \"lr\": float(lr),\n",
        "        \"batch_size\": float(bs),\n",
        "        \"epochs\": float(ep),\n",
        "        \"weight_decay\": float(wd),\n",
        "        \"best_val_macro_f1\": float(val_f1),\n",
        "    }\n",
        "    results.append(row)\n",
        "\n",
        "    if val_f1 > best_val_f1_overall:\n",
        "        best_val_f1_overall = float(val_f1)\n",
        "        best_config = {\n",
        "            \"lr\": float(lr),\n",
        "            \"batch_size\": float(bs),\n",
        "            \"epochs\": float(ep),\n",
        "            \"weight_decay\": float(wd),\n",
        "        }\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values(\"best_val_macro_f1\", ascending=False)\n",
        "\n",
        "print(\"\\n✅ Best config (VAL Macro-F1):\", best_config)\n",
        "print(\"Best VAL Macro-F1:\", best_val_f1_overall)\n",
        "\n",
        "grid_path = os.path.join(CFG.output_dir, \"grid_results.csv\")\n",
        "results_df.to_csv(grid_path, index=False)\n",
        "print(\"Saved:\", grid_path)\n",
        "\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOB-gQOdKJNd"
      },
      "source": [
        "## **Retrain best config once + save history + save best checkpoint**\n",
        "\n",
        "We retrain using the best hyperparameters and record:\n",
        "- train loss\n",
        "- validation loss\n",
        "- validation Macro-F1\n",
        "\n",
        "**Saved artifacts:**\n",
        "- `history.csv`\n",
        "- `best_model.pt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtkuSvc6MX6F"
      },
      "outputs": [],
      "source": [
        "def train_bert_with_history(\n",
        "    model_name: str,\n",
        "    num_labels: int,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    learning_rate: float,\n",
        "    epochs: int,\n",
        "    weight_decay: float,\n",
        "    warmup_ratio: float,\n",
        "    patience: int,\n",
        "    grad_clip_norm: float,\n",
        "    seed: int,\n",
        "    grad_accum_steps: int = 1,\n",
        "    use_amp: bool = True,\n",
        ") -> Tuple[Dict[str, torch.Tensor], pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"Train model, log history, and return best checkpoint based on val macro-F1.\"\"\"\n",
        "    set_seed(seed)\n",
        "    model = build_model(model_name, num_labels).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # Correct scheduler step count when using gradient accumulation\n",
        "    steps_per_epoch = (len(train_loader) + grad_accum_steps - 1) // grad_accum_steps\n",
        "    total_steps = steps_per_epoch * epochs\n",
        "    warmup_steps = int(warmup_ratio * total_steps)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "\n",
        "    history: List[Dict[str, float]] = []\n",
        "    best_val_f1 = -1.0\n",
        "    best_epoch = 0\n",
        "    best_state_dict: Optional[Dict[str, torch.Tensor]] = None\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train_one_epoch(\n",
        "            model=model,\n",
        "            dataloader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            grad_clip_norm=grad_clip_norm,\n",
        "            grad_accum_steps=grad_accum_steps,\n",
        "            use_amp=use_amp,\n",
        "        )\n",
        "\n",
        "        val_out = evaluate_model(model, val_loader)\n",
        "        val_loss = float(val_out[\"loss\"])\n",
        "        val_f1 = float(val_out[\"metrics\"][\"macro_f1\"])\n",
        "\n",
        "        history.append(\n",
        "            {\n",
        "                \"epoch\": float(epoch),\n",
        "                \"train_loss\": float(train_loss),\n",
        "                \"val_loss\": float(val_loss),\n",
        "                \"val_macro_f1\": float(val_f1),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{epochs} | \"\n",
        "            f\"train_loss={train_loss:.4f} val_loss={val_loss:.4f} val_macro_f1={val_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            best_epoch = epoch\n",
        "            best_state_dict = {\n",
        "                k: v.detach().cpu().clone() for k, v in model.state_dict().items()\n",
        "            }\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping triggered (no improvement on val Macro-F1).\")\n",
        "            break\n",
        "\n",
        "    if best_state_dict is None:\n",
        "        best_state_dict = {\n",
        "            k: v.detach().cpu().clone() for k, v in model.state_dict().items()\n",
        "        }\n",
        "\n",
        "    history_df = pd.DataFrame(history)\n",
        "    best_info = {\"best_epoch\": int(best_epoch), \"best_val_macro_f1\": float(best_val_f1)}\n",
        "\n",
        "    return best_state_dict, history_df, best_info\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Retrain best config + save\n",
        "# ----------------------------\n",
        "if best_config is None:\n",
        "    raise ValueError(\"Grid search found no valid config (maybe all OOM).\")\n",
        "\n",
        "best_lr = float(best_config[\"lr\"])\n",
        "best_bs = int(best_config[\"batch_size\"])\n",
        "best_epochs = int(best_config[\"epochs\"])\n",
        "best_wd = float(best_config[\"weight_decay\"])\n",
        "\n",
        "clear_gpu_memory()\n",
        "\n",
        "train_loader, val_loader, test_loader, _ = prepare_tokenized_splits_and_loaders(\n",
        "    ds=dataset,\n",
        "    model_name=CFG.model_name,\n",
        "    max_length=CFG.max_length,\n",
        "    batch_size=best_bs,\n",
        "    val_ratio=CFG.val_ratio,\n",
        "    seed=CFG.seed,\n",
        ")\n",
        "\n",
        "best_state_dict, history_df, best_info = train_bert_with_history(\n",
        "    model_name=CFG.model_name,\n",
        "    num_labels=num_labels,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    learning_rate=best_lr,\n",
        "    epochs=best_epochs,\n",
        "    weight_decay=best_wd,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    patience=patience,\n",
        "    grad_clip_norm=grad_clip_norm,\n",
        "    seed=CFG.seed,\n",
        "    grad_accum_steps=grad_accum_steps,  # uses your global setting (usually 1)\n",
        "    use_amp=use_amp,                    # uses your global setting (usually True)\n",
        ")\n",
        "\n",
        "print(\"\\nBest epoch:\", best_info[\"best_epoch\"])\n",
        "print(\"Best VAL Macro-F1:\", best_info[\"best_val_macro_f1\"])\n",
        "\n",
        "history_path = os.path.join(CFG.output_dir, \"history.csv\")\n",
        "history_df.to_csv(history_path, index=False)\n",
        "print(\"Saved:\", history_path)\n",
        "\n",
        "best_model_path = os.path.join(CFG.output_dir, \"best_model.pt\")\n",
        "torch.save(best_state_dict, best_model_path)\n",
        "print(\"Saved:\", best_model_path)\n",
        "\n",
        "history_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBRkvlLTKUBn"
      },
      "source": [
        "## **Plot training curves**\n",
        "\n",
        "We plot:\n",
        "- Train vs validation loss\n",
        "- Validation Macro-F1 across epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODomxCKa8BNj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_training_curves_one_figure(\n",
        "    history: pd.DataFrame,\n",
        "    *,\n",
        "    use_log_loss: bool = True,\n",
        "    save_path: str = \"bert_training_curves_one_figure.png\",\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    One-figure training curves for reports\n",
        "\n",
        "    What it shows (self-explanatory):\n",
        "    - Train vs Val loss (optionally Log(loss)) on left y-axis\n",
        "    - Validation Macro-F1 on right y-axis\n",
        "    - Best epoch marker (highest val_macro_f1)\n",
        "    - Compact legend, readable fonts, high-DPI export\n",
        "    \"\"\"\n",
        "    required_cols = {\"epoch\", \"train_loss\", \"val_loss\", \"val_macro_f1\"}\n",
        "    missing = required_cols - set(history.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"history is missing columns: {sorted(missing)}\")\n",
        "\n",
        "    df = history.copy()\n",
        "    df = df.sort_values(\"epoch\")\n",
        "\n",
        "    train_loss = df[\"train_loss\"].astype(float).to_numpy()\n",
        "    val_loss = df[\"val_loss\"].astype(float).to_numpy()\n",
        "    val_f1 = df[\"val_macro_f1\"].astype(float).to_numpy()\n",
        "    epochs = df[\"epoch\"].astype(int).to_numpy()\n",
        "\n",
        "    if use_log_loss:\n",
        "        y_train = np.log(np.maximum(train_loss, 1e-12))\n",
        "        y_val = np.log(np.maximum(val_loss, 1e-12))\n",
        "        loss_ylabel = \"Log(loss)\"\n",
        "        title = \"BERT fine-tuning learning curves (Log loss + Validation Macro-F1)\"\n",
        "    else:\n",
        "        y_train = train_loss\n",
        "        y_val = val_loss\n",
        "        loss_ylabel = \"Loss\"\n",
        "        title = \"BERT fine-tuning learning curves (Loss + Validation Macro-F1)\"\n",
        "\n",
        "    best_idx = int(np.argmax(val_f1))\n",
        "    best_epoch = int(epochs[best_idx])\n",
        "    best_f1 = float(val_f1[best_idx])\n",
        "\n",
        "    # Word 2-column friendly size + readable fonts\n",
        "    title_fs = 10\n",
        "    label_fs = 11\n",
        "    tick_fs = 11\n",
        "    legend_fs = 9\n",
        "    annot_fs = 9\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(6, 3))\n",
        "\n",
        "    # Loss curves on left axis\n",
        "    l1 = ax1.plot(epochs, y_train, label=\"Train loss\", linewidth=2)\n",
        "    l2 = ax1.plot(epochs, y_val, label=\"Validation loss\", linewidth=2)\n",
        "\n",
        "    ax1.set_title(title, fontsize=title_fs, pad=8)\n",
        "    ax1.set_xlabel(\"Epoch\", fontsize=label_fs)\n",
        "    ax1.set_ylabel(loss_ylabel, fontsize=label_fs)\n",
        "    ax1.tick_params(axis=\"both\", labelsize=tick_fs)\n",
        "\n",
        "    # F1 curve on right axis\n",
        "    ax2 = ax1.twinx()\n",
        "    l3 = ax2.plot(epochs, val_f1, label=\"Validation Macro-F1\", linewidth=2)\n",
        "    ax2.set_ylabel(\"Macro-F1\", fontsize=label_fs)\n",
        "    ax2.tick_params(axis=\"y\", labelsize=tick_fs)\n",
        "    ax2.set_ylim(0.0, 1.0)\n",
        "\n",
        "    # Best epoch marker (clear and report-friendly)\n",
        "    ax2.axvline(best_epoch, linestyle=\"--\", linewidth=1)\n",
        "    ax2.scatter([best_epoch], [best_f1], s=35)\n",
        "    ax2.text(\n",
        "        best_epoch,\n",
        "        best_f1,\n",
        "        f\"  best @ epoch {best_epoch} (F1={best_f1:.3f})\",\n",
        "        fontsize=annot_fs,\n",
        "        va=\"center\",\n",
        "    )\n",
        "\n",
        "    # Compact legend combining both axes\n",
        "    lines = l1 + l2 + l3\n",
        "    labels = [ln.get_label() for ln in lines]\n",
        "    ax1.legend(\n",
        "        lines,\n",
        "        labels,\n",
        "        loc=\"lower right\",\n",
        "        fontsize=legend_fs,\n",
        "        frameon=True,\n",
        "        borderpad=0.3,\n",
        "        labelspacing=0.3,\n",
        "        handlelength=1.2,\n",
        "        handletextpad=0.4,\n",
        "    )\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(save_path, dpi=400, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(f\"Saved figure to: {save_path}\")\n",
        "\n",
        "\n",
        "# Use ONE call only (recommended in the report)\n",
        "plot_training_curves_one_figure(history_df, use_log_loss=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqvIoKLbKaWK"
      },
      "source": [
        "## **Final evaluation on official TEST set (used once)**\n",
        "\n",
        "**Important:** The test set is used only here, after selecting the model on validation and save `test_predictions.csv` for reproducibility/audit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrEk6WERoxRQ"
      },
      "outputs": [],
      "source": [
        "def final_test_evaluation_and_save_predictions(\n",
        "    model_name: str,\n",
        "    num_labels: int,\n",
        "    best_state_dict: Dict[str, torch.Tensor],\n",
        "    test_loader: DataLoader,\n",
        "    label_names: List[str],\n",
        "    output_dir: str,\n",
        ") -> Tuple[float, Dict[str, float], np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Evaluate the selected best checkpoint on the official TEST set once and save predictions.\n",
        "\n",
        "    Returns:\n",
        "        test_loss: Weighted average test loss.\n",
        "        test_metrics: Dict with accuracy + macro precision/recall/F1.\n",
        "        y_true: True labels.\n",
        "        y_pred: Predicted labels.\n",
        "    \"\"\"\n",
        "    model = build_model(model_name, num_labels).to(device)\n",
        "    model.load_state_dict(best_state_dict)\n",
        "\n",
        "    test_out = evaluate_model(model, test_loader)\n",
        "    test_loss = float(test_out[\"loss\"])\n",
        "    test_metrics = test_out[\"metrics\"]\n",
        "    y_true = test_out[\"y_true\"]\n",
        "    y_pred = test_out[\"y_pred\"]\n",
        "\n",
        "    print(\"FINAL BERT results (OFFICIAL TEST) — used once:\")\n",
        "    print(f\"{'test_loss':>16}: {test_loss:.4f}\")\n",
        "    for k, v in test_metrics.items():\n",
        "        print(f\"{k:>16}: {float(v):.4f}\")\n",
        "\n",
        "    print(\"\\nClassification report (BERT):\")\n",
        "    print(classification_report(y_true, y_pred, target_names=label_names))\n",
        "\n",
        "    preds_path = os.path.join(output_dir, \"test_predictions.csv\")\n",
        "    pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred}).to_csv(preds_path, index=False)\n",
        "    print(\"Saved:\", preds_path)\n",
        "\n",
        "    return test_loss, test_metrics, y_true, y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVcxHD7lCkKl"
      },
      "source": [
        "### **Call the Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nthSfogUozpK"
      },
      "outputs": [],
      "source": [
        "test_loss, test_metrics, y_true, y_pred = final_test_evaluation_and_save_predictions(\n",
        "    model_name=CFG.model_name,\n",
        "    num_labels=num_labels,\n",
        "    best_state_dict=best_state_dict,\n",
        "    test_loader=test_loader,\n",
        "    label_names=label_names,\n",
        "    output_dir=CFG.output_dir,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn8IK3PEKhSC"
      },
      "source": [
        "## **Confusion matrix (BERT)**\n",
        "\n",
        "This visualizes which classes are most commonly confused on the official test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPYARuS9pGnt"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(\n",
        "    y_true_arr: np.ndarray,\n",
        "    y_pred_arr: np.ndarray,\n",
        "    class_names: List[str],\n",
        "    title: str,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Plot and return the confusion matrix.\n",
        "\n",
        "    Returns:\n",
        "        cm: Confusion matrix array of shape (num_classes, num_classes).\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true_arr, y_pred_arr)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.imshow(cm)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.xticks(range(len(class_names)), class_names, rotation=30, ha=\"right\")\n",
        "    plt.yticks(range(len(class_names)), class_names)\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6riE88F5wSc"
      },
      "outputs": [],
      "source": [
        "cm_bert = plot_confusion_matrix(\n",
        "    y_true_arr=y_true,\n",
        "    y_pred_arr=y_pred,\n",
        "    class_names=label_names,\n",
        "    title=\"Confusion Matrix (Fine-tuned BERT)\",\n",
        ")\n",
        "\n",
        "cm_bert\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apMjd090Kmnz"
      },
      "source": [
        "## **Baseline vs Fine-tuned BERT comparison**\n",
        "\n",
        "We compare Macro-F1 and accuracy between:\n",
        "- TF-IDF + Logistic Regression baseline\n",
        "- Fine-tuned BERT (selected on validation Macro-F1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMd_7PkZpKXJ"
      },
      "outputs": [],
      "source": [
        "def build_comparison_table(\n",
        "    baseline_metrics: Dict[str, float],\n",
        "    bert_metrics: Dict[str, float],\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a comparison table for baseline vs BERT using key metrics.\n",
        "\n",
        "    Returns:\n",
        "        comparison_df: DataFrame sorted by macro_f1 descending.\n",
        "    \"\"\"\n",
        "    comparison_df = pd.DataFrame(\n",
        "        [\n",
        "            {\n",
        "                \"model\": \"Baseline (TF-IDF + Logistic Regression)\",\n",
        "                **baseline_metrics,\n",
        "            },\n",
        "            {\n",
        "                \"model\": \"Fine-tuned BERT (selected by VAL Macro-F1)\",\n",
        "                **bert_metrics,\n",
        "            },\n",
        "        ]\n",
        "    ).sort_values(\"macro_f1\", ascending=False)\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "\n",
        "comparison_df = build_comparison_table(\n",
        "    baseline_metrics=baseline_metrics,\n",
        "    bert_metrics=test_metrics,\n",
        ")\n",
        "\n",
        "comparison_path = os.path.join(CFG.output_dir, \"comparison.csv\")\n",
        "comparison_df.to_csv(comparison_path, index=False)\n",
        "print(\"Saved:\", comparison_path)\n",
        "\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBgYgA2GJ7rL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def plot_model_comparison_bars_aesthetic(\n",
        "    baseline_metrics: dict,\n",
        "    bert_metrics: dict,\n",
        "    *,\n",
        "    title: str = \"Model comparison on AG News (Test set)\",\n",
        "    save_path: str = \"model_comparison_bar_percent.png\",\n",
        "    figsize=(6, 4.5),\n",
        "    dpi: int = 400,\n",
        "    as_percent: bool = True\n",
        ") -> None:\n",
        "    metrics_order = [\"accuracy\", \"macro_precision\", \"macro_recall\", \"macro_f1\"]\n",
        "    metric_labels = [\"Accuracy\", \"Macro-Precision\", \"Macro-Recall\", \"Macro-F1\"]\n",
        "\n",
        "    baseline_vals = np.array([float(baseline_metrics[m]) for m in metrics_order])\n",
        "    bert_vals = np.array([float(bert_metrics[m]) for m in metrics_order])\n",
        "\n",
        "    if as_percent:\n",
        "        baseline_plot = baseline_vals * 100\n",
        "        bert_plot = bert_vals * 100\n",
        "        y_label = \"Score (%)\"\n",
        "\n",
        "        # ✅ Add headroom so % labels don’t touch the top border\n",
        "        ymax = max(baseline_plot.max(), bert_plot.max())\n",
        "        ax_top = min(120, ymax + 8)   # usually gives nice space (e.g., 94.9 -> 105.9)\n",
        "        y_lim = (0, ax_top)\n",
        "    else:\n",
        "        baseline_plot = baseline_vals\n",
        "        bert_plot = bert_vals\n",
        "        y_label = \"Score\"\n",
        "        y_lim = (0, 1.05)\n",
        "\n",
        "    x = np.arange(len(metrics_order))\n",
        "    width = 0.40\n",
        "\n",
        "    baseline_color = \"#2A9D8F\"  # teal\n",
        "    bert_color = \"#E76F51\"      # coral\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.set_axisbelow(True)\n",
        "    ax.grid(axis=\"y\", linestyle=\"--\", linewidth=0.8, alpha=0.30)\n",
        "\n",
        "    bars1 = ax.bar(x - width/2, baseline_plot, width, label=\"Baseline (TF-IDF + LR)\",\n",
        "                   color=baseline_color, edgecolor=\"black\", linewidth=0.35)\n",
        "    bars2 = ax.bar(x + width/2, bert_plot, width, label=\"Fine-tuned BERT\",\n",
        "                   color=bert_color, edgecolor=\"black\", linewidth=0.35)\n",
        "\n",
        "    ax.set_title(title, fontsize=13, pad=10)\n",
        "    ax.set_ylabel(y_label, fontsize=12)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metric_labels, fontsize=12)\n",
        "    ax.tick_params(axis=\"y\", labelsize=12)\n",
        "    ax.set_ylim(*y_lim)\n",
        "\n",
        "    # ✅ Slightly smaller offset + enough space above\n",
        "    def annotate(bars):\n",
        "      for b in bars:\n",
        "        h = float(b.get_height())\n",
        "\n",
        "        if as_percent:\n",
        "            # ✅ show rounded-to-1dp (94.9 → 95.0 if actual is 94.95+)\n",
        "            text = f\"{round(h, 1):.2f}%\"\n",
        "            offset = 0.8\n",
        "        else:\n",
        "            text = f\"{h:.2f}\"\n",
        "            offset = 0.2\n",
        "\n",
        "        ax.text(\n",
        "            b.get_x() + b.get_width()/2,\n",
        "            h + offset,\n",
        "            text,\n",
        "            ha=\"center\",\n",
        "            va=\"bottom\",\n",
        "            fontsize=10\n",
        "        )\n",
        "    annotate(bars1)\n",
        "    annotate(bars2)\n",
        "\n",
        "    ax.legend(fontsize=10, frameon=True, framealpha=0.95, loc=\"lower right\")\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(\"Saved figure to:\", save_path)\n",
        "\n",
        "\n",
        "# Usage:\n",
        "plot_model_comparison_bars_aesthetic(\n",
        "    baseline_metrics=baseline_metrics,\n",
        "    bert_metrics=test_metrics,\n",
        "    as_percent=True,\n",
        "    save_path=os.path.join(CFG.output_dir, \"model_comparison_bar_percent.png\"),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjNAmInyKybx"
      },
      "source": [
        "## **Error analysis (misclassified examples)**\n",
        "\n",
        "To support interpretation, we inspect misclassified samples:\n",
        "- shows the text snippet\n",
        "- true label vs predicted label\n",
        "\n",
        "This helps discuss limitations and future improvements in the report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ2AnHeBpOlA"
      },
      "outputs": [],
      "source": [
        "def get_misclassified_examples(\n",
        "    ds_test,\n",
        "    y_true_arr: np.ndarray,\n",
        "    y_pred_arr: np.ndarray,\n",
        "    label_names: List[str],\n",
        "    max_examples: int = 20,\n",
        "    text_chars: int = 200,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build a table of misclassified test examples.\n",
        "\n",
        "    Args:\n",
        "        ds_test: Raw test split containing \"text\".\n",
        "        y_true_arr: True label IDs.\n",
        "        y_pred_arr: Pred label IDs.\n",
        "        label_names: Mapping from label ID to label string.\n",
        "        max_examples: Max number of misclassified rows to return.\n",
        "        text_chars: Number of text characters to show for each example.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with misclassified samples (index, true/pred labels, text snippet).\n",
        "    \"\"\"\n",
        "    wrong_idx = np.where(y_true_arr != y_pred_arr)[0]\n",
        "    wrong_idx = wrong_idx[:max_examples]\n",
        "\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    for i in wrong_idx:\n",
        "        text = ds_test[int(i)][\"text\"]\n",
        "        rows.append(\n",
        "            {\n",
        "                \"test_index\": int(i),\n",
        "                \"true_id\": int(y_true_arr[i]),\n",
        "                \"true_label\": label_names[int(y_true_arr[i])],\n",
        "                \"pred_id\": int(y_pred_arr[i]),\n",
        "                \"pred_label\": label_names[int(y_pred_arr[i])],\n",
        "                \"text_snippet\": text[:text_chars].replace(\"\\n\", \" \"),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtsihuE36LoS"
      },
      "outputs": [],
      "source": [
        "misclassified_df = get_misclassified_examples(\n",
        "    ds_test=dataset[\"test\"],\n",
        "    y_true_arr=y_true,\n",
        "    y_pred_arr=y_pred,\n",
        "    label_names=label_names,\n",
        "    max_examples=20,\n",
        "    text_chars=250,\n",
        ")\n",
        "\n",
        "misclassified_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdwH6szf6ODD"
      },
      "outputs": [],
      "source": [
        "misclassified_path = os.path.join(CFG.output_dir, \"misclassified_examples.csv\")\n",
        "misclassified_df.to_csv(misclassified_path, index=False)\n",
        "print(\"Saved:\", misclassified_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}